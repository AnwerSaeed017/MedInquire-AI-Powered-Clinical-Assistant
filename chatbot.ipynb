{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7xr9bNkU46P"
      },
      "outputs": [],
      "source": [
        "!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "HF_API_KEY = userdata.get(\"HF_API_KEY\")\n",
        "os.environ[\"HF_API_KEY\"] = \"HF_API_KEY\""
      ],
      "metadata": {
        "id": "zYZPil-xVkFw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS, Chroma\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "import pathlib\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace('â€¢', '  *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "5SX7MevZcg3c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader"
      ],
      "metadata": {
        "id": "ynRAop0Zfj8F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader(\n",
        "    '/content/drive/My Drive/medical_data',\n",
        "    glob=\"**/*.txt\",\n",
        "    loader_cls=TextLoader\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "FNXpy7EvfnSY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader('/content/drive/My Drive/medical_data')\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "Y6I8AYzWWI27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "HrLUij3lWnhC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "id": "oss6APp-WtVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chroma vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "# Test query\n",
        "query = \"who is at risk of heart disease\"\n",
        "search = vectorstore.similarity_search(query, k=3)\n",
        "print(search[0].page_content)"
      ],
      "metadata": {
        "id": "I2jVoa9mf1rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={'k': 5}\n",
        ")\n",
        "retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "f0DD_MjQW8yp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Mc5DoRsJ2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "998ZpbbItet8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"BioMistral/BioMistral-7B-GGUF\",\n",
        "    filename=\"ggml-model-Q4_K_M.gguf\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=512,\n",
        "    top_p=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "4hXH3ad2W_j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "template = \"\"\"\n",
        "<|context|>\n",
        "You are an AI assistant that follows instruction extremely well.\n",
        "Please be truthful and give direct answers\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        " <|assistant|>\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "rag_chain = (\n",
        "    {'context': retriever, 'query': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "h1AYa4LzXJsP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the prompt template with clear instructions.\n",
        "prompt_template = \"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Instructions:\n",
        "Based on the above context (which may include overlapping or redundant text), please provide a concise and clear answer to the question.\n",
        "Summarize and deduplicate any repeated information. Focus only on the key details relevant to the question.\n",
        "Answer:\n",
        "\"\"\"\n",
        "chat_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define your query\n",
        "query = \"What radiotherapy treatment was recommended for the patient with brain metastases?\"\n",
        "\n",
        "# Retrieve documents from the vector store retriever\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "# Combine the retrieved text (using a set for deduplication)\n",
        "context = \"\\n\\n\".join({doc.page_content for doc in retrieved_docs})\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = chat_prompt.format(context=context, query=query)\n",
        "\n",
        "# Generate a response using your LLM (e.g., llm, loaded using Llama.from_pretrained)\n",
        "response = llm(formatted_prompt)\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "SZ1hALUAvA_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNsgdHhTy4W5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}